# -*- coding: utf-8 -*-
"""Tutorial LightFm_PDBSCAN Variaveis textuais Inglês 2024

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KksEb6QvylOgbKSUoo76Pk0MnOHmt4NH
"""

from google.colab import drive
drive.mount('/content/gdrive')
gdrive_path = '/content/gdrive/My Drive/Karla/'

pip install scikit_optimize

pip install LightFM

from sklearn.preprocessing import StandardScaler # Import the StandardScaler class
!pip install hdbscan
import hdbscan # Import the hdbscan library

# import dependent libraries
import sklearn
import pandas as pd
import os
from scipy.sparse import csr_matrix
import numpy as np
from IPython.display import display_html
import warnings

import matplotlib.pyplot as plt
from matplotlib.gridspec import GridSpec
import seaborn as sns
#%matplotlib inline
from lightfm import LightFM
from lightfm.cross_validation import random_train_test_split
from lightfm.evaluation import auc_score, precision_at_k, recall_at_k, reciprocal_rank
from lightfm import LightFM, cross_validation
from skopt import forest_minimize


def display_side_by_side(*args):
    html_str = ''
    for df in args:
        html_str += df.to_html()
    display_html(html_str.replace(
        'table', 'table style="display:inline"'), raw=True)

# update the working directory to the root of the project
os.chdir('..')
warnings.filterwarnings("ignore")

"""DATABASE DESC_METADATA"""

desc_metadata = pd.read_csv('/content/gdrive/MyDrive/Colab Notebooks/BD_FINAL.csv', encoding='latin1', names=['id_local',	'nome',	'categoria',	'descr',	'rating_geral',	'verificado', 'latitude', 'longitude'], skiprows=2)
desc_metadata.head()



desc_metadata.dtypes

rating_user = pd.read_csv('/content/gdrive/MyDrive/rating_filtrados_ingles_en.csv', encoding ='latin_1', names=['user_id', 'id_local', 'rating'])
rating_user.head()

"""# Acrescentar a base de dados das categorias, para depois converter todas em minuscúla"""

categories = pd.read_csv('/content/gdrive/MyDrive/Foursquare_Categories.csv', sep=';')
categories.head()

desc_metadata['categoria_minuscula'] = desc_metadata.categoria.apply(lambda x: x.lower())
desc_metadata.describe(include='all')

categories[(categories['Level1'].str.lower().str.contains("brewery")) |
           (categories['Level2'].str.lower().str.contains("brewery")) |
           (categories['Level3'].str.lower().str.contains("brewery")) ]

#From categories taxonomy to a dictionary
#Structure: {'categoryName':'rootCategory', ....}

categories['Category_Label']=categories['Category_Label'].astype(str)
categories['Level1']=categories['Level1'].astype(str)
categories['Level2']=categories['Level2'].astype(str)
categories['Level3']=categories['Level3'].astype(str)

categories_dic = {}
for _,row in categories.iterrows():
    category = row['Category_Label'].strip().lower()
    categories_dic[category] = category
    if row['Level1'].lower().strip() != 'nan':
        l1 = row['Level1'].strip().lower()
        categories_dic[l1] = category
    if row['Level2'].lower() != 'nan':
        l2 = row['Level2'].strip().lower()
        categories_dic[l2] = category
    if row['Level3'].lower() != 'nan':
        l3 = row['Level3'].strip().lower()
        categories_dic[l3] = category

print(categories_dic)

"""# Vamos criar uma nova coluna com a Raiz da categoria CATEGORIA ROOT"""

desc_metadata['root']=desc_metadata.categoria_minuscula.map(categories_dic, na_action='ignore')
desc_metadata.head(100)

#Remover Zeros
desc_metadata.shape

desc_metadata = desc_metadata.dropna()
desc_metadata.shape

"""# distribuição entre as diferentes categorias raiz"""

print(desc_metadata.groupby('root').size())
desc_metadata.shape

desc_metadata = desc_metadata[desc_metadata['root'] != 'community and government']
# Verificar se a categoria foi removida e o novo tamanho do DataFrame
print(desc_metadata.groupby('root').size())
print(desc_metadata.shape)

"""## Gráfico"""

import seaborn as sns
import matplotlib.pyplot as plt
sns.countplot(data=desc_metadata,y="root", label="Count")
plt.show()

"""**Incorpora-se a categoria, nome e descrição para cada POI utilizando a norma para cada título do vetor. Este vetor também é incorporado no dataframe**
---
# Biblioteca SPACY

***Serão trasnformados os textos a minuscúla***

---



---
"""

!python3 -m pip install -U spacy
import spacy
!python3 -m spacy download en_core_web_md

"""# Transformar  vetor: Categoria







"""

nlp = spacy.load('en_core_web_md')

# Preparar listas para armazenar as normas e vetores
norms = []
vectors = []

# Iterar sobre as categorias e processar o texto
for cat in desc_metadata['categoria_minuscula'].values:
    doc = nlp(cat)
    if doc.has_vector:
        vectors.append(doc.vector)
        norms.append(doc.vector_norm)
    else:
        # Caso o documento não tenha vetor, adicionar vetores e normas nulas
        vectors.append(np.zeros(nlp.vocab.vectors.shape[1]))
        norms.append(0.0)

# Converter a lista de vetores em um array numpy
vectors_array = np.array(vectors)

# Criar DataFrame com vetores
vectors_df = pd.DataFrame(vectors_array, columns=[f"dim_{i}" for i in range(vectors_array.shape[1])])

# Adicionar colunas ao DataFrame original
desc_metadata['category_norm'] = norms
desc_metadata_new = pd.concat([desc_metadata, vectors_df], axis=1)

desc_metadata_new.head()

desc_metadata_new.shape

desc_metadata_new = desc_metadata_new.dropna()
desc_metadata_new.shape

"""# Transformar  vetor: Nome

"""

desc_metadata["nome_minuscula"] = desc_metadata.nome.apply(lambda x: x.lower())
desc_metadata.head()

# Preparar listas para armazenar as normas e vetores
norms = []
vectors = []

# Iterar sobre os nomes e processar o texto
for nom in desc_metadata['nome_minuscula'].values:
    doc = nlp(nom)
    if doc.has_vector:
        vectors.append(doc.vector)
        norms.append(doc.vector_norm)
    else:
        # Caso o documento não tenha vetor, adicionar vetores e normas nulas
        vectors.append(np.zeros(nlp.vocab.vectors.shape[1]))
        norms.append(0.0)

# Converter a lista de vetores em um array numpy
vectors_array = np.array(vectors)

# Criar DataFrame com vetores
vectors_df = pd.DataFrame(vectors_array, columns=[f"dim_{i}" for i in range(vectors_array.shape[1])])

# Adicionar colunas ao DataFrame original
desc_metadata['name_norm'] = norms
desc_metadata_new2 = pd.concat([desc_metadata, vectors_df], axis=1)

desc_metadata_new2.head()

desc_metadata.reindex()

desc_metadata_new2 = desc_metadata_new2.dropna()
desc_metadata_new2.shape

"""# Transformar  vetor: Descrição

"""

desc_metadata["desc_minuscula"] = desc_metadata.descr.apply(lambda x: x.lower())
desc_metadata.head()

# Carregar o modelo do SpaCy
nlp = spacy.load('en_core_web_md')

# Preparar listas para armazenar as normas e vetores
norms = []
vectors = []

# Iterar sobre as descrições e processar o texto
for desc in desc_metadata["desc_minuscula"].values:
    doc = nlp(desc)
    if doc.has_vector:
        vectors.append(doc.vector)
        norms.append(doc.vector_norm)
    else:
        # Caso o documento não tenha vetor, adicionar vetores e normas nulas
        vectors.append(np.zeros(nlp.vocab.vectors.shape[1]))
        norms.append(0.0)

# Converter a lista de vetores em um array numpy
vectors_array = np.array(vectors)

# Criar DataFrame com vetores
vectors_df = pd.DataFrame(vectors_array, columns=[f"dim_{i}" for i in range(vectors_array.shape[1])])

# Adicionar colunas ao DataFrame original
desc_metadata["descr_norm"] = norms
desc_metadata_new3 = pd.concat([desc_metadata, vectors_df], axis=1)

desc_metadata_new3.head()

"""# Transformar vetor: Root"""

# Carregar o modelo do SpaCy
nlp = spacy.load('en_core_web_md')

# Preparar listas para armazenar as normas e vetores
norms = []
vectors = []

# Iterar sobre as descrições e processar o texto
for cat_root in desc_metadata["root"].values:
    doc = nlp(cat_root)
    if doc.has_vector:
        vectors.append(doc.vector)
        norms.append(doc.vector_norm)
    else:
        # Caso o documento não tenha vetor, adicionar vetores e normas nulas
        vectors.append(np.zeros(nlp.vocab.vectors.shape[1]))
        norms.append(0.0)

# Converter a lista de vetores em um array numpy
vectors_array = np.array(vectors)

# Criar DataFrame com vetores
vectors_df = pd.DataFrame(vectors_array, columns=[f"dim_{i}" for i in range(vectors_array.shape[1])])

# Adicionar colunas ao DataFrame original
desc_metadata["root_norm"] = norms
desc_metadata_new4 = pd.concat([desc_metadata, vectors_df], axis=1)

desc_metadata_new4.head()

desc_metadata_new4.dtypes

desc_metadata.dtypes



"""# Vamo-nos concentrar apenas nos campos selecionados que requerem manipulação mínima.

"""

desc_metadata_selected= desc_metadata[['id_local','name_norm','descr_norm','category_norm','root_norm','rating_geral','verificado','latitude', 'longitude']]

"""vamos executá-los por meio do pandas profiler para realizar uma análise preliminar de dados exploratórios para nos ajudar a entender melhor os dados disponíveis

"""

!pip install sweetviz

import sweetviz as sv

# Gerar o relatório do sweetviz
report = sv.analyze((desc_metadata_selected[['name_norm', 'descr_norm', 'category_norm', 'rating_geral', 'verificado', 'root_norm', 'latitude', 'longitude']]), feat_cfg=sv.FeatureConfig (force_num=['root_norm']))
report.show_html('/content/sweetviz_report.html')

"""Converter valores bin para variáveis ​​numéricas em intervalos discretos

"""

#Mapeo booleano para string
booleanDictionary = {True: 'VERDADEIRO', False: 'FALSO'}
desc_metadata_selected['verificado'] = desc_metadata_selected['verificado'].replace(booleanDictionary)
# converter a coluna verified em 1/0 onde true=1 e false=0
desc_metadata_selected['verificado'] = desc_metadata_selected.verificado.map(
    lambda x: 1.0*(x == 'true'))

report = sv.analyze((desc_metadata_selected[['name_norm', 'descr_norm', 'category_norm', 'rating_geral', 'verificado', 'root_norm', 'latitude', 'longitude']]), feat_cfg=sv.FeatureConfig (force_num=['root_norm']))

report.show_html('/content/sweetviz_report3.html')

desc_metadata_selected.sample(5)

"""# Para disminuir a quantidade de Zeros no dataframe é preciso criar intervalos

**Pd.cut criar os intervalos rating_geral**
"""

#Converter a intervalos  para diminuir o número de colunas

bin = [4.40, 4.83, 5.25, 5.68, 6.10, 6.53, 6.95, 7.38, 7.80, 8.23, 8.65, 9.08, 9.50]

int_1 = pd.cut(desc_metadata_selected.rating_geral,bin)
int_1 =int_1.to_frame()
int_1.columns = ['intervalo_rating']

#concatenar o dataframe com a nova coluna intervalo_1
df0_new_desc_metadata_selected = pd.concat([desc_metadata_selected,int_1],axis = 1)
df0_new_desc_metadata_selected .shape
df0_new_desc_metadata_selected .head()

"""# Pd.cut criar os intervalos para a coluna name_norm


"""

#Converter a intervalos  para diminuir o número de colunas

bin = [0.0, 0.8275, 1.655, 2.4825, 3.31, 4.1375, 4.965, 5.7925, 6.62, 7.4475, 8.275, 9.1025, 9.93]

int_2 = pd.cut(desc_metadata_selected.name_norm,bin)
int_2 =int_2.to_frame()
int_2.columns = ['intervalo_nome']

#concatenar o dataframe com a nova coluna intervalo_2
df1_new_desc_metadata_selected = pd.concat([df0_new_desc_metadata_selected ,int_2],axis = 1)
df1_new_desc_metadata_selected.shape
df1_new_desc_metadata_selected.head()

"""# **Pd.cut criar os intervalos para a coluna desc_norm**"""

#Converter a intervalos  para diminuir o número de colunas

bin = [0.0, 0.57833, 1.15667, 1.735, 2.31333, 2.89167, 3.47, 4.04833, 4.62667, 5.205, 5.78333, 6.36167, 6.94]

int_3 = pd.cut(desc_metadata_selected.descr_norm,bin)
int_3 =int_3.to_frame()
int_3.columns = ['intervalo_descr']

#concatenar o dataframe com a nova coluna intervalo_3
df2_new_desc_metadata_selected = pd.concat([df0_new_desc_metadata_selected,int_3],axis = 1)
df2_new_desc_metadata_selected.shape
df2_new_desc_metadata_selected.head()

#concatenar o dataframe com a nova coluna intervalo_3
df2_2new_desc_metadata_selected = pd.concat([df1_new_desc_metadata_selected,int_3],axis = 1)
df2_2new_desc_metadata_selected.shape
df2_2new_desc_metadata_selected.head()

"""# **Pd.cut criar os intervalos para a coluna category_norm**"""

#Converter a intervalos  para diminuir o número de colunas

bin = [3.81, 4.23333, 4.65667, 5.08, 5.50333, 5.92667, 6.35, 6.77333, 7.19667, 7.62, 8.04333, 8.46667, 8.89]

int_4 = pd.cut(desc_metadata_selected.category_norm,bin)
int_4 =int_4.to_frame()
int_4.columns = ['intervalo_category']

#concatenar o dataframe com a nova coluna intervalo_3
df3_new_desc_metadata_selected = pd.concat([df2_2new_desc_metadata_selected,int_4],axis = 1)
df3_new_desc_metadata_selected.shape
df3_new_desc_metadata_selected.head()

#concatenar o dataframe com a nova coluna intervalo_3
df3_3_new_desc_metadata_selected = pd.concat([df3_new_desc_metadata_selected,int_4],axis = 1)
df3_3_new_desc_metadata_selected.shape
df3_3_new_desc_metadata_selected.head()

"""# **Pd.cut criar os intervalos para a coluna root norm **\"""

#Converter a intervalos  para diminuir o número de colunas

bin = [4.4, 4.625, 4.8, 4.975, 5.15, 5.325, 5.5, 5.675, 5.85, 6.025, 6.2, 6.375, 6.55]

int_5 = pd.cut(desc_metadata_selected.root_norm,bin)
int_5 =int_5.to_frame()
int_5.columns = ['intervalo_root']

#concatenar o dataframe com a nova coluna intervalo_4
df4_new_desc_metadata_selected = pd.concat([df3_new_desc_metadata_selected,int_5],axis = 1)
df4_new_desc_metadata_selected.shape
df4_new_desc_metadata_selected.head()

#concatenar o dataframe com a nova coluna intervalo_3
df4_4_new_desc_metadata_selected = pd.concat([df1_new_desc_metadata_selected,int_5],axis = 1)
df4_4_new_desc_metadata_selected.shape
df4_4_new_desc_metadata_selected.head()

"""#DBSCAN"""

# Import the DBSCAN class from sklearn.cluster
from sklearn.cluster import DBSCAN

# Aplicar DBSCAN nas coordenadas de latitude e longitude
lat_long = df4_new_desc_metadata_selected[['latitude', 'longitude']].values

# Ajuste de eps e min_samples conforme o contexto dos dados
dbscan = DBSCAN(eps=0.01, min_samples=5)
clusters = dbscan.fit_predict(lat_long)

# Adicionar os clusters ao DataFrame
df4_new_desc_metadata_selected['cluster'] = clusters

# Verificar os clusters criados
print(df4_new_desc_metadata_selected['cluster'].value_counts())

# Remover a coluna original de 'latitude	longitude' se não for mais necessária
df4_new_desc_metadata_selected.drop([ 'latitude',	'longitude'], axis=1, inplace=True)

"""**DATABASE RATING_USER**"""

rating_user.columns.values

rating_user.sample(5)

rating_user.shape

# Utilizar os dados disponíveis
rating_user_selected = rating_user[['user_id', 'id_local','rating']]

"""'''Agora, vamos transformar os dados disponíveis em matriz esparsa CSR que pode ser usada
 para operações de matriz ada linha contém os pesos dos locais sobre os recursos.
 No entanto, antes de criarmos uma matriz esparsa, primeiro criaremos um dicionário
 de itens para referências futuras'''

# Data Preprocessing
"""

#Data Preprocessing

item_dict ={}
df = desc_metadata[['id_local', 'nome']].sort_values('id_local').reset_index()

for i in range(df.shape[0]):
    item_dict[(df.loc[i,'id_local'])] = df.loc[i,'nome']

# simular características categóricas
desc_metadata_selected_transformed = pd.get_dummies(df4_new_desc_metadata_selected, columns=['intervalo_rating','verificado', 'cluster','intervalo_category','intervalo_nome','intervalo_root','intervalo_descr'])
desc_metadata_selected_transformed = desc_metadata_selected_transformed.sort_values('id_local').reset_index().drop('index', axis=1)

# Remover a coluna 'id_local' e converter todas as colunas para valores numéricos
desc_metadata_selected_transformed_numeric = desc_metadata_selected_transformed.drop('id_local', axis=1)

# Garantir que todas as colunas tenham tipos numéricos
desc_metadata_selected_transformed_numeric = desc_metadata_selected_transformed_numeric.astype(float)
desc_metadata_selected_transformed.head(5)

# Criar a matriz CSR
desc_metadata_csr = csr_matrix(desc_metadata_selected_transformed_numeric.values)

# Exibir informações da matriz CSR
print(desc_metadata_csr)

#Em seguida, construir  uma matriz de iterações que é np.float64 csr_matrix de forma ([n_users, n_locais]).
user_desc_rating = pd.pivot_table(rating_user_selected, index='user_id', columns='id_local', values='rating')

# preencher os valores que faltam com 0
user_desc_rating = user_desc_rating.fillna(0)

user_desc_rating.head(10)

#criars um dicionário do utilizador para casos de uso futuros

user_id = list(user_desc_rating.index)
user_dict = {}
counter = 0
for i in user_id:
    user_dict[i] = counter
    counter += 1

# convertir para matrix csr
user_desc_rating_csr = csr_matrix(user_desc_rating.values)
user_desc_rating_csr

#split
train, test = cross_validation.random_train_test_split(user_desc_rating_csr, test_percentage=0.3)

model = LightFM(loss='bpr',
                random_state=2016,
                learning_rate=0.75,
                no_components=50,
                user_alpha=0.0001,
                item_alpha=0.0001)

model = model.fit(user_desc_rating_csr,
                  item_features=desc_metadata_csr,
                  epochs=30,
                  num_threads=8, verbose=False)

#MODEL TRAINING
model = LightFM(loss='warp',
                random_state=2016,
                learning_rate=0.50,
                no_components=150,
                user_alpha=0.000005)

model = model.fit(user_desc_rating_csr,
                  item_features=desc_metadata_csr,
                  epochs=50,
                  num_threads=16, verbose=False)

# Top n Recommendations
def sample_recommendation_user(model, rating_user, user_id, user_dict, desc_metadata,
                               item_dict, desc_metadata_csr, threshold=0, nrec_items=5, show=True):

    # Veiricar o número de utilizadores e PPOIS
    n_users, n_items = rating_user.shape
     # Certificar de que user_id é um único valor
    if isinstance(user_id, list):
        user_id = user_id[0]  # Use o primeiro elemento

    # Obter o índice do utilizador
    user_x = user_dict.get(user_id, None)
    if user_x is None:
        print(f"Utilizador {user_id} não encontrado no dicionário de utilizadores.")
        return [], []

    # Se user_x não for uma estrutura adequada, obtem a linha correspondente
    user_x_vector = rating_user.iloc[user_x].values if isinstance(user_x, int) else user_x


    # Tentar prever os scores
    try:
        scores = pd.Series(model.predict(user_x_vector, np.arange(n_items), item_features=desc_metadata_csr))
    except Exception as e:
        print(f"Erro ao prever scores: {e}")
        return [], []
    # Ordenar os scores
    scores.index = rating_user.columns
    scores = list(pd.Series(scores.sort_values(ascending=False).index))

    # Filtrar os itens já conhecidos (visitados)
    known_items = list(pd.Series(rating_user.loc[user_id, :] \
                                 [rating_user.loc[user_id, :] > threshold].index).sort_values(ascending=False))

    # Garantir que os itens conhecidos não sejam recomendados
    scores = [x for x in scores if x not in known_items]
    return_score_list = scores[:nrec_items]

    # Obter as categorias e categorias raiz
    known_items_info = []
    for item in known_items:
        poi_data = desc_metadata[desc_metadata['id_local'] == item]
        if not poi_data.empty:
            poi_data = poi_data.iloc[0]
            known_items_info.append((item_dict[item], poi_data['categoria'], poi_data['root']))

    scores_info = []
    for item in return_score_list:
        poi_data = desc_metadata[desc_metadata['id_local'] == item]
        if not poi_data.empty:
            poi_data = poi_data.iloc[0]
            scores_info.append((item_dict[item], poi_data['categoria'], poi_data['root']))

    if show:
        print("Utilizador: " + str(user_id))
        print("Onde foi:")
        counter = 1
        for name, category, root in known_items_info:
            print(f"{counter}- {name} (Categoria: {category} - Categoria raiz: {root})")
            counter += 1

        print("\nRecomendaçao de Locais:\n")
        counter = 1
        for name, category, root in scores_info:
            print(f"{counter}- {name} (Categoria: {category} - Categoria raiz: {root})")
            counter += 1



# Retornar as informações obtidas, ou None se algo der errado
    return known_items_info, scores_info

# Chamada da função
sample_recommendation_user(model, user_desc_rating, 2028795, user_dict, desc_metadata, item_dict, desc_metadata_csr)

"""*Número de utilizadores: 861: Isso significa que há 861 utilizadores diferentes que interagiram com os itens do recomendação.
Número de itens: 28: Isso indica que há 28  POIs para avaliar ou que estão disponíveis para recomendação.*
"""

#avaliar para observar a precisão
train_precision = precision_at_k(model, train, k=5, item_features=desc_metadata_csr).mean()
test_precision = precision_at_k(model, test, k=5, train_interactions=train, item_features=desc_metadata_csr).mean()

# Avaliar para observar AUC
train_auc = auc_score(model, train, item_features=desc_metadata_csr).mean()  # Correção aqui
test_auc = auc_score(model, test, train_interactions=train, item_features=desc_metadata_csr).mean()  # Correção aqui

# Avaliar para observar Recall
recall_train = recall_at_k(model, train, k=5, item_features=desc_metadata_csr).mean()  # Correção aqui
recall_test = recall_at_k(model, test, k=5, item_features=desc_metadata_csr, train_interactions=train).mean()  # Correção aqui

# Avaliar para observar Mean Reciprocal Rank (MRR)
mrr_train = reciprocal_rank(model, train, item_features=desc_metadata_csr).mean()  # Correção aqui
mrr_test = reciprocal_rank(model, test, item_features=desc_metadata_csr, train_interactions=train).mean()  # Correção aqui

print('Precision: train %.2f, test %.2f.' % (train_precision, test_precision))
print('AUC: train %.2f, test %.2f.' % (train_auc, test_auc))
print('Recall: train %.2f, test %.2f.' % (recall_train, recall_test))
print('MRR: train %.2f, test %.2f.' % (mrr_train, mrr_test))

"""# Recomendação para os primeiros 100 utilizadores"""

import random

# Carregar o CSV com os 100 utilizadores
users_df = pd.read_csv('/content/gdrive/MyDrive/user_ids_top_100_usuarios_diversos.csv')

# Função para treinar e ajustar o modelo
def train_model_per_user(model, user_desc_rating_csr, desc_metadata_csr, user_id, user_interactions):
    """
    Treinar o modelo LightFM usando as interações do utilizador atual.
    """
    # Atualizar os dados do CSR com base no utilizador atual (exemplo de ajuste incremental)
    user_interactions_csr = user_interactions  # Supõe-se que user_interactions seja um CSR matrix para o utilizador atual
    updated_user_item_matrix = user_desc_rating_csr.copy()  # Copiar o CSR original
    updated_user_item_matrix[user_id, :] = user_interactions_csr  # Atualizar as interações do utilizador

    # Re-treinar o modelo com os dados atualizados
    model.fit(updated_user_item_matrix, item_features=desc_metadata_csr, epochs=10, num_threads=16, verbose=False)

    return model

# Função para gerar recomendações
def generate_user_recommendations_csv(users_file, model, user_desc_rating_csr, desc_metadata_csr, user_dict, desc_metadata, item_dict, threshold=0, nrec_items=5):
    users_df = pd.read_csv(users_file)
    columns = ["id_user",
               "visitou 1", "categoria 1", "root 1",
               "visitou 2", "categoria 2", "root 2",
               "visitou 3", "categoria 3", "root 3",
               "visitou 4", "categoria 4", "root 4",
               "visitou 5", "categoria 5", "root 5",
               "Recomendação 1", "categoria r 1", "root r 1",
               "Recomendação 2", "categoria r 2", "root r 2",
               "Recomendação 3", "categoria r 3", "root r 3",
               "Recomendação 4", "categoria r 4", "root r 4",
               "Recomendação 5", "categoria r 5", "root r 5"]

    result_data = []

    for user_id in users_df['user_id']:
        print(f"Processando utilizador {user_id}")

        # Obter as interações do utilizador atual (substituir pelo método adequado)
        user_interactions = user_desc_rating_csr[user_dict[user_id], :]

        # Treinar o modelo para o utilizador atual
        model = train_model_per_user(model, user_desc_rating_csr, desc_metadata_csr, user_dict[user_id], user_interactions)

        # Gerar recomendações
        known_items_info, scores_info = sample_recommendation_user(
            model, user_desc_rating, user_id, user_dict, desc_metadata, item_dict, desc_metadata_csr, threshold, nrec_items)

        # Criar linha de dados
        row = [user_id]

        # Preencher os locais visitados
        for i in range(nrec_items):
            if i < len(known_items_info):
                visit_name, visit_category, visit_root = known_items_info[i]
                row.extend([visit_name, visit_category, visit_root])
            else:
                row.extend(["", "", ""])

        # Preencher as recomendações
        for i in range(nrec_items):
            if i < len(scores_info):
                rec_name, rec_category, rec_root = scores_info[i]
                row.extend([rec_name, rec_category, rec_root])
            else:
                row.extend(["", "", ""])

        result_data.append(row)

    # Salvar os resultados em CSV
    result_df = pd.DataFrame(result_data, columns=columns)
    result_df.to_csv('Rec_100_Utilizadores_Com_Treinamento_ingles.csv', index=False, encoding='utf-8-sig')

    print("CSV gerado com sucesso!")

# Configuração inicial
users_file = "/content/gdrive/MyDrive/user_ids_top_100_usuarios_diversos.csv"
generate_user_recommendations_csv(users_file, model, user_desc_rating_csr, desc_metadata_csr, user_dict, desc_metadata, item_dict)