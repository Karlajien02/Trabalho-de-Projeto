# -*- coding: utf-8 -*-
"""28/01  Tutorial LightFm_ Variaveis textuais portugûes 2024

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18zfGxjnwBxiQAdRAToLj2XS8jiZWeTKu
"""

from google.colab import drive
drive.mount('/content/gdrive')
gdrive_path = '/content/gdrive/My Drive/Karla/'

pip install scikit_optimize

pip install LightFM

# import dependent libraries
import sklearn
import pandas as pd
import os
from scipy.sparse import csr_matrix
import numpy as np
from IPython.display import display_html
import warnings

import matplotlib.pyplot as plt
from matplotlib.gridspec import GridSpec
import seaborn as sns
#%matplotlib inline
from lightfm import LightFM
from lightfm.cross_validation import random_train_test_split
from lightfm.evaluation import auc_score, precision_at_k, recall_at_k, reciprocal_rank
from lightfm import LightFM, cross_validation
from skopt import forest_minimize

def display_side_by_side(*args):
    html_str = ''
    for df in args:
        html_str += df.to_html()
    display_html(html_str.replace(
        'table', 'table style="display:inline"'), raw=True)

# update the working directory to the root of the project
os.chdir('..')
warnings.filterwarnings("ignore")

"""DATABASE DESC_METADATA"""

#pois = pd.read_csv('/content/gdrive/MyDrive/POIS_TODOS_PT.csv')
#pois.head()

desc_metadata = pd.read_csv('/content/locales_descr_portugues.csv', encoding='utf-8', names=['id_local',	'nome',	'categoria',	'descr',	'rating_geral',	'verificado', 'latitude', 'longitude'], skiprows=1)
desc_metadata.head()

desc_metadata.dtypes

rating_user = pd.read_csv('/content/gdrive/MyDrive/Colab Notebooks/rating_filtrados_portugues.csv', encoding ='latin_1', names=['user_id', 'id_local', 'rating'])
rating_user.head()

"""# Acrescentar a base de dados das categorias, para depois converter todas em minuscúla"""

categories = pd.read_csv('/content/gdrive/MyDrive/Foursquare_Categories.csv', sep=';')
categories.head()

desc_metadata['categoria_minuscula'] = desc_metadata.categoria.apply(lambda x: x.lower())
desc_metadata.describe(include='all')

categories[(categories['Level1'].str.lower().str.contains("brewery")) |
           (categories['Level2'].str.lower().str.contains("brewery")) |
           (categories['Level3'].str.lower().str.contains("brewery")) ]

#From categories taxonomy to a dictionary
#Structure: {'categoryName':'rootCategory', ....}

categories['Category_Label']=categories['Category_Label'].astype(str)
categories['Level1']=categories['Level1'].astype(str)
categories['Level2']=categories['Level2'].astype(str)
categories['Level3']=categories['Level3'].astype(str)

categories_dic = {}
for _,row in categories.iterrows():
    category = row['Category_Label'].strip().lower()
    categories_dic[category] = category
    if row['Level1'].lower().strip() != 'nan':
        l1 = row['Level1'].strip().lower()
        categories_dic[l1] = category
    if row['Level2'].lower() != 'nan':
        l2 = row['Level2'].strip().lower()
        categories_dic[l2] = category
    if row['Level3'].lower() != 'nan':
        l3 = row['Level3'].strip().lower()
        categories_dic[l3] = category

print(categories_dic)

"""# Vamos criar uma nova coluna com a Raiz da categoria CATEGORIA ROOT"""

desc_metadata['root']=desc_metadata.categoria_minuscula.map(categories_dic, na_action='ignore')
desc_metadata.head(100)

#Remover Zeros
desc_metadata.shape

desc_metadata = desc_metadata.dropna()
desc_metadata.shape

"""# distribuição entre as diferentes categorias raiz"""

print(desc_metadata.groupby('root').size())
desc_metadata.shape

"""## Gráfico"""

import seaborn as sns
import matplotlib.pyplot as plt
sns.countplot(data=desc_metadata,y="root", label="Count")
plt.show()

"""**Incorporamos o nome da categoria, Nome e descrição para cada POI utilizando a norma para cada título do vetor. Este vetor também é incorporado no dataframe**
---
# Biblioteca SPACY

***Serão trasnformados os textos a minuscúla***

---



---
"""

!python3 -m pip install -U spacy
import spacy
!python -m spacy download pt_core_news_md

"""# Transformar  vetor: Categoria







"""

nlp = spacy.load('/usr/local/lib/python3.10/dist-packages/pt_core_news_md/pt_core_news_md-3.8.0')

norms=[]
vectors=np.zeros((1035,300))
i=0
for cat in desc_metadata['categoria_minuscula'].values:
    doc = nlp(cat)
    if doc.has_vector:
        vectors[i,:]=doc.vector
    norms.append(doc.vector_norm)
    i = i + 1

desc_metadata.insert(2, 'category_norm',  norms)
desc_metadata_new = pd.concat([desc_metadata, pd.DataFrame(vectors, columns=["0", "1", "2", "3", "4", "5", "6", "7", "8", "9", "10", "11", "12", "13", "14", "15", "16", "17", "18", "19", "20", "21", "22", "23", "24", "25", "26", "27", "28", "29", "30", "31", "32", "33", "34", "35", "36", "37", "38", "39", "40", "41", "42", "43", "44", "45", "46", "47", "48", "49", "50", "51", "52", "53", "54", "55", "56", "57", "58", "59", "60", "61", "62", "63", "64", "65", "66", "67", "68", "69", "70", "71", "72", "73", "74", "75", "76", "77", "78", "79", "80", "81", "82", "83", "84", "85", "86", "87", "88", "89", "90", "91", "92", "93", "94", "95", " 96", "97", "98", "99", "100", "101", "102", "103", "104", "105", "106", "107", "108", "109", "110", "111", "112", "113", "114", "115", "116", "117", "118", "119", "120", "121", "122", "123", "124", "125", "126", "127", "128", "129", "130", "131", "132", "133", "134", "135", "136", "137", "138", "139", "140", "141", "142", "143", "144", "145", "146", "147", "148", "149", "150", "151", "152", "153", "154", "155", "156", "157", "158", "159", "160", "161", "162", "163", "164", "165", "166", "167", "168", "169", "170", "171", "172", "173", "174", "175", "176", "177", "178", "179", "180", "181", "182", "183", "184", "185", "186", "187", "188", "189", "190", "191", "192", "193", "194", "195", "196", "197", "198", "199", "200", "201", "202", "203", "204", "205", "206", "207", "208", "209", "210", "211", "212", "213", "214", "215", "216", "217", "218", "219", "220", "221", "222", "223", "224", "225", "226", "227", "228", "229", "230", "231", "232", "233", "234", "235", "236", "237", "238", "239", "240", "241", "242", "243", "244", "245", "246", "247", "248", "249", "250", "251", "252", "253", "254", "255", "256", "257", "258", "259", "260", "261", "262", "263", "264", "265", "266", "267", "268", "269", "270", "271", "272", "273", "274", "275", "276", "277", "278", "279", "280", "281", "282", "283", "284", "285", "286", "287", "288", "289", "290", "291", "292", "293", "294", "295", "296", "297", "298", "299"])], axis=1, join='inner')
desc_metadata_new.head()

desc_metadata_new.shape

desc_metadata_new = desc_metadata_new.dropna()
desc_metadata_new.shape

"""# Transformar  vetor: Nome

"""

desc_metadata["nome_minuscula"] = desc_metadata.nome.apply(lambda x: x.lower())
desc_metadata.head()

norms=[]
vectors=np.zeros((1035,300))
i=0
for nom in desc_metadata['nome_minuscula'].values:
    doc = nlp(nom)
    if doc.has_vector:
      vectors[i,:]=doc.vector
    norms.append(doc.vector_norm)
    i = i + 1

desc_metadata.insert(2, 'name_norm',  norms)
desc_metadata_new2 = pd.concat([desc_metadata, pd.DataFrame(vectors, columns=["0", "1", "2", "3", "4", "5", "6", "7", "8", "9", "10", "11", "12", "13", "14", "15", "16", "17", "18", "19", "20", "21", "22", "23", "24", "25", "26", "27", "28", "29", "30", "31", "32", "33", "34", "35", "36", "37", "38", "39", "40", "41", "42", "43", "44", "45", "46", "47", "48", "49", "50", "51", "52", "53", "54", "55", "56", "57", "58", "59", "60", "61", "62", "63", "64", "65", "66", "67", "68", "69", "70", "71", "72", "73", "74", "75", "76", "77", "78", "79", "80", "81", "82", "83", "84", "85", "86", "87", "88", "89", "90", "91", "92", "93", "94", "95", " 96", "97", "98", "99", "100", "101", "102", "103", "104", "105", "106", "107", "108", "109", "110", "111", "112", "113", "114", "115", "116", "117", "118", "119", "120", "121", "122", "123", "124", "125", "126", "127", "128", "129", "130", "131", "132", "133", "134", "135", "136", "137", "138", "139", "140", "141", "142", "143", "144", "145", "146", "147", "148", "149", "150", "151", "152", "153", "154", "155", "156", "157", "158", "159", "160", "161", "162", "163", "164", "165", "166", "167", "168", "169", "170", "171", "172", "173", "174", "175", "176", "177", "178", "179", "180", "181", "182", "183", "184", "185", "186", "187", "188", "189", "190", "191", "192", "193", "194", "195", "196", "197", "198", "199", "200", "201", "202", "203", "204", "205", "206", "207", "208", "209", "210", "211", "212", "213", "214", "215", "216", "217", "218", "219", "220", "221", "222", "223", "224", "225", "226", "227", "228", "229", "230", "231", "232", "233", "234", "235", "236", "237", "238", "239", "240", "241", "242", "243", "244", "245", "246", "247", "248", "249", "250", "251", "252", "253", "254", "255", "256", "257", "258", "259", "260", "261", "262", "263", "264", "265", "266", "267", "268", "269", "270", "271", "272", "273", "274", "275", "276", "277", "278", "279", "280", "281", "282", "283", "284", "285", "286", "287", "288", "289", "290", "291", "292", "293", "294", "295", "296", "297", "298", "299"])], axis=1, join='inner')
desc_metadata_new2.head()

desc_metadata.reindex()

"""# Transformar  vetor: Descrição

"""

desc_metadata["desc_minuscula"] = desc_metadata.descr.apply(lambda x: x.lower())
desc_metadata.head()

norms=[]
vectors=np.zeros((1035,300))
i=0
for desc in desc_metadata["desc_minuscula"].values:
    doc = nlp(desc)
    if doc.has_vector:
      vectors[i,:]=doc.vector
    norms.append(doc.vector_norm)
    i = i + 1

desc_metadata.insert(2, "descr_norm",  norms)
desc_metadata_new3 = pd.concat([desc_metadata, pd.DataFrame(vectors, columns=["0", "1", "2", "3", "4", "5", "6", "7", "8", "9", "10", "11", "12", "13", "14", "15", "16", "17", "18", "19", "20", "21", "22", "23", "24", "25", "26", "27", "28", "29", "30", "31", "32", "33", "34", "35", "36", "37", "38", "39", "40", "41", "42", "43", "44", "45", "46", "47", "48", "49", "50", "51", "52", "53", "54", "55", "56", "57", "58", "59", "60", "61", "62", "63", "64", "65", "66", "67", "68", "69", "70", "71", "72", "73", "74", "75", "76", "77", "78", "79", "80", "81", "82", "83", "84", "85", "86", "87", "88", "89", "90", "91", "92", "93", "94", "95", " 96", "97", "98", "99", "100", "101", "102", "103", "104", "105", "106", "107", "108", "109", "110", "111", "112", "113", "114", "115", "116", "117", "118", "119", "120", "121", "122", "123", "124", "125", "126", "127", "128", "129", "130", "131", "132", "133", "134", "135", "136", "137", "138", "139", "140", "141", "142", "143", "144", "145", "146", "147", "148", "149", "150", "151", "152", "153", "154", "155", "156", "157", "158", "159", "160", "161", "162", "163", "164", "165", "166", "167", "168", "169", "170", "171", "172", "173", "174", "175", "176", "177", "178", "179", "180", "181", "182", "183", "184", "185", "186", "187", "188", "189", "190", "191", "192", "193", "194", "195", "196", "197", "198", "199", "200", "201", "202", "203", "204", "205", "206", "207", "208", "209", "210", "211", "212", "213", "214", "215", "216", "217", "218", "219", "220", "221", "222", "223", "224", "225", "226", "227", "228", "229", "230", "231", "232", "233", "234", "235", "236", "237", "238", "239", "240", "241", "242", "243", "244", "245", "246", "247", "248", "249", "250", "251", "252", "253", "254", "255", "256", "257", "258", "259", "260", "261", "262", "263", "264", "265", "266", "267", "268", "269", "270", "271", "272", "273", "274", "275", "276", "277", "278", "279", "280", "281", "282", "283", "284", "285", "286", "287", "288", "289", "290", "291", "292", "293", "294", "295", "296", "297", "298", "299"])], axis=1, join='inner')
desc_metadata_new3.head()

"""# Transformar vetor: Root"""

norms=[]
vectors=np.zeros((1035,300))
i=0
for cat_root in desc_metadata["root"].values:
    doc = nlp(cat_root)
    if doc.has_vector:
      vectors[i,:]=doc.vector
    norms.append(doc.vector_norm)
    i = i + 1

desc_metadata.insert(2, "root_norm",  norms)
desc_metadata_new4 = pd.concat([desc_metadata, pd.DataFrame(vectors, columns=["0", "1", "2", "3", "4", "5", "6", "7", "8", "9", "10", "11", "12", "13", "14", "15", "16", "17", "18", "19", "20", "21", "22", "23", "24", "25", "26", "27", "28", "29", "30", "31", "32", "33", "34", "35", "36", "37", "38", "39", "40", "41", "42", "43", "44", "45", "46", "47", "48", "49", "50", "51", "52", "53", "54", "55", "56", "57", "58", "59", "60", "61", "62", "63", "64", "65", "66", "67", "68", "69", "70", "71", "72", "73", "74", "75", "76", "77", "78", "79", "80", "81", "82", "83", "84", "85", "86", "87", "88", "89", "90", "91", "92", "93", "94", "95", " 96", "97", "98", "99", "100", "101", "102", "103", "104", "105", "106", "107", "108", "109", "110", "111", "112", "113", "114", "115", "116", "117", "118", "119", "120", "121", "122", "123", "124", "125", "126", "127", "128", "129", "130", "131", "132", "133", "134", "135", "136", "137", "138", "139", "140", "141", "142", "143", "144", "145", "146", "147", "148", "149", "150", "151", "152", "153", "154", "155", "156", "157", "158", "159", "160", "161", "162", "163", "164", "165", "166", "167", "168", "169", "170", "171", "172", "173", "174", "175", "176", "177", "178", "179", "180", "181", "182", "183", "184", "185", "186", "187", "188", "189", "190", "191", "192", "193", "194", "195", "196", "197", "198", "199", "200", "201", "202", "203", "204", "205", "206", "207", "208", "209", "210", "211", "212", "213", "214", "215", "216", "217", "218", "219", "220", "221", "222", "223", "224", "225", "226", "227", "228", "229", "230", "231", "232", "233", "234", "235", "236", "237", "238", "239", "240", "241", "242", "243", "244", "245", "246", "247", "248", "249", "250", "251", "252", "253", "254", "255", "256", "257", "258", "259", "260", "261", "262", "263", "264", "265", "266", "267", "268", "269", "270", "271", "272", "273", "274", "275", "276", "277", "278", "279", "280", "281", "282", "283", "284", "285", "286", "287", "288", "289", "290", "291", "292", "293", "294", "295", "296", "297", "298", "299"])], axis=1, join='inner')
desc_metadata_new4.head()

desc_metadata.reindex()

"""# Vamo-nos concentrar apenas nos campos selecionados que requerem manipulação mínima.

"""

!pip install sweetviz

import sweetviz as sv

# Gerar o relatório do sweetviz
report = sv.analyze((desc_metadata[['name_norm', 'descr_norm', 'category_norm', 'rating_geral', 'verificado', 'root_norm', 'latitude', 'longitude']]), feat_cfg=sv.FeatureConfig (force_num=['root_norm']))
report.show_html('/content/sweetviz_report.html')



"""vamos executá-los por meio do pandas profiler para realizar uma análise preliminar de dados exploratórios para nos ajudar a entender melhor os dados disponíveis

Converter valores bin para variáveis ​​numéricas em intervalos discretos
"""

#Mapeo booleano para string
booleanDictionary = {True: 'VERDADEIRO', False: 'FALSO'}
desc_metadata_selected['verificado'] = desc_metadata_selected['verificado'].replace(booleanDictionary)
# converter a coluna verified em 1/0 onde true=1 e false=0
desc_metadata_selected['verificado'] = desc_metadata_selected.verificado.map(
    lambda x: 1.0*(x == 'true'))

#novo profiler
profile = pandas_profiling.ProfileReport(desc_metadata_selected[['name_norm','descr_norm','category_norm','rating_geral','verificado','root_norm']])
profile.to_file('/content/profiler_desc_metadata_2.html')

desc_metadata_selected.sample(5)

"""# Para disminuir a quantidade de Zeros no dataframe é preciso criar rangos

**Pd.cut criar os rangos**
"""

#Converter a rangos  para diminuir o número de colunas

bin = [4.4,4.9,5.4,5.9,6.4,6.9,7.4,7.9,8.4,8.9,9.4,9.9]

int_1 = pd.cut(desc_metadata_selected.rating_geral,bin)
int_1 =int_1.to_frame()
int_1.columns = ['intervalo_rating']

#concatenar o dataframe com a nova coluna intervalo_1
df0_new_desc_metadata_selected = pd.concat([desc_metadata_selected,int_1],axis = 1)
df0_new_desc_metadata_selected .shape
df0_new_desc_metadata_selected .head()

"""Pd.cut criar os intervalos para a coluna name_norm

**negrito**
"""

#Converter a intervalos  para diminuir o número de colunas

bin = [0, 5.80, 11.60, 17.40,23.20,29.009,34.80, 40.60, 46.40,52.20, 58.009, 63.61]

int_2 = pd.cut(desc_metadata_selected.name_norm,bin)
int_2 =int_2.to_frame()
int_2.columns = ['intervalo_nome']

#concatenar o dataframe com a nova coluna intervalo_2
df1_new_desc_metadata_selected = pd.concat([df0_new_desc_metadata_selected ,int_2],axis = 1)
df1_new_desc_metadata_selected.shape
df1_new_desc_metadata_selected.head()

"""Pd.cut criar os intervalos para a coluna desc_norm"""

#Converter a intervalos  para diminuir o número de colunas

bin = [12.204, 13.924, 15.644, 17.364, 19.084, 20.804, 22.524, 24.244, 25.964, 27.684, 29.404, 31.124]

int_3 = pd.cut(desc_metadata_selected.descr_norm,bin)
int_3 =int_3.to_frame()
int_3.columns = ['intervalo_descr']

#concatenar o dataframe com a nova coluna intervalo_3
df2_new_desc_metadata_selected = pd.concat([df0_new_desc_metadata_selected,int_3],axis = 1)
df2_new_desc_metadata_selected.shape
df2_new_desc_metadata_selected.head()

#concatenar o dataframe com a nova coluna intervalo_3
df2_2new_desc_metadata_selected = pd.concat([df1_new_desc_metadata_selected,int_3],axis = 1)
df2_2new_desc_metadata_selected.shape
df2_2new_desc_metadata_selected.head()

"""**Pd.cut criar os intervalos para a coluna category_norm**"""

#Converter a intervalos  para diminuir o número de colunas

bin = [0, 4.52, 9.04, 13.56, 18.08, 22.60, 27.126, 31.64, 36.16, 40.68, 45.20, 49.72]

int_4 = pd.cut(desc_metadata_selected.category_norm,bin)
int_4 =int_4.to_frame()
int_4.columns = ['intervalo_category']

#concatenar o dataframe com a nova coluna intervalo_3
df3_new_desc_metadata_selected = pd.concat([df0_new_desc_metadata_selected,int_4],axis = 1)
df3_new_desc_metadata_selected.shape
df3_new_desc_metadata_selected.head()

#concatenar o dataframe com a nova coluna intervalo_3
df3_3_new_desc_metadata_selected = pd.concat([df1_new_desc_metadata_selected,int_4],axis = 1)
df3_3_new_desc_metadata_selected.shape
df3_3_new_desc_metadata_selected.head()

"""Pd.cut criar os intervalos para a coluna root_norm"""

#Converter a intervalos  para diminuir o número de colunas

bin = [19.37, 20.77, 21.47, 22.17, 22.87, 23.57, 24.27, 24.97, 25.67, 26.37, 27.07, 27.77]

int_5 = pd.cut(desc_metadata_selected.root_norm,bin)
int_5 =int_5.to_frame()
int_5.columns = ['intervalo_root']

#concatenar o dataframe com a nova coluna intervalo_3
df4_new_desc_metadata_selected = pd.concat([df0_new_desc_metadata_selected,int_5],axis = 1)
df4_new_desc_metadata_selected.shape
df4_new_desc_metadata_selected.head()

#concatenar o dataframe com a nova coluna intervalo_3
df4_4_new_desc_metadata_selected = pd.concat([df1_new_desc_metadata_selected,int_5],axis = 1)
df4_4_new_desc_metadata_selected.shape
df4_4_new_desc_metadata_selected.head()

"""**DATABASE RATING_USER**"""

rating_user.columns.values

rating_user.sample(5)

rating_user.shape

# Utilizar os dados disponíveis
rating_user_selected = rating_user[['user_id', 'id_local','rating']]

"""'''Agora, vamos transformar os dados disponíveis em matriz esparsa CSR que pode ser usada
 para operações de matriz ada linha contém os pesos dos locais sobre os recursos.
 No entanto, antes de criarmos uma matriz esparsa, primeiro criaremos um dicionário
 de itens para referências futuras'''
"""

#Data Preprocessing

item_dict ={}
df = desc_metadata[['id_local', 'nome']].sort_values('id_local').reset_index()

for i in range(df.shape[0]):
    item_dict[(df.loc[i,'id_local'])] = df.loc[i,'nome']

# simular características categóricas
desc_metadata_selected_transformed = pd.get_dummies(df3_3_new_desc_metadata_selected,columns = ['intervalo_rating','verificado', 'intervalo_nome', 'intervalo_category'])

desc_metadata_selected_transformed = desc_metadata_selected_transformed.sort_values('id_local').reset_index().drop('index', axis=1)


#  convertir para matrix csr
desc_metadata_csr = csr_matrix(desc_metadata_selected_transformed.drop('id_local', axis=1).values)
desc_metadata_csr

#Em seguida, construir  uma matriz de iterações que é np.float64 csr_matrix de forma ([n_users, n_locais]).
user_desc_rating = pd.pivot_table(rating_user_selected, index='user_id', columns='id_local', values='rating')

desc_metadata_selected_transformed.head(5)

#  convertir para matrix csr
desc_metadata_csr = csr_matrix(desc_metadata_selected_transformed.drop('id_local', axis=1).values)
desc_metadata_csr

#Em seguida, construir  uma matriz de iterações que é np.float64 csr_matrix de forma ([n_users, n_locais]).
user_desc_rating = pd.pivot_table(rating_user_selected, index='user_id', columns='id_local', values='rating')

# preencher os valores que faltam com 0
user_desc_rating = user_desc_rating.fillna(0)

user_desc_rating.head(10)

#criars um dicionário do utilizador para casos de uso futuros

user_id = list(user_desc_rating.index)
user_dict = {}
counter = 0
for i in user_id:
    user_dict[i] = counter
    counter += 1

# convertir para matrix csr
user_desc_rating_csr = csr_matrix(user_desc_rating.values)
user_desc_rating_csr

#split
train, test = cross_validation.random_train_test_split(user_desc_rating_csr, test_percentage=0.3)

#MODEL TRAINING
model = LightFM(loss='warp',
                random_state=2016,
                learning_rate=0.50,
                no_components=150,
                user_alpha=0.000005)

model = model.fit(user_desc_rating_csr,
                  item_features=user_desc_rating_csr,
                  epochs=50,
                  num_threads=16, verbose=False)

#Top n Recommendations
def sample_recommendation_user(model, rating_user, user_id, user_dict,
                               item_dict,threshold = 0,nrec_items = 5, show = True):

    n_users, n_items = rating_user.shape
    user_x = user_dict[user_id]
    scores = pd.Series(model.predict(user_x,np.arange(n_items), item_features=desc_metadata_csr))
    scores.index = rating_user.columns
    scores = list(pd.Series(scores.sort_values(ascending=False).index))

    known_items = list(pd.Series(rating_user.loc[user_id,:] \
                                 [rating_user.loc[user_id,:] > threshold].index).sort_values(ascending=False))

    scores = [x for x in scores if x not in known_items]
    return_score_list = scores[0:nrec_items]
    known_items = list(pd.Series(known_items).apply(lambda x: item_dict[x]))
    scores = list(pd.Series(return_score_list).apply(lambda x: item_dict[x]))
    if show == True:
        print ("User: " + str(user_id))
        print("Known Likes:")
        counter = 1
        for i in known_items:
            print(str(counter) + '- ' + i)
            counter+=1

        print("\n Recommended Items:\n")
        counter = 1
        for i in scores:
            print(str(counter) + '- ' + i)
            counter+=1

sample_recommendation_user(model, user_desc_rating, 553092 , user_dict, item_dict)

#avaliar para observar a precisão
train_precision = precision_at_k(model, train, k=10, item_features=user_desc_rating_csr).mean()
test_precision = precision_at_k(model, test, k=10, train_interactions=train, item_features=user_desc_rating_csr).mean()

#avaliar para observar auc
train_auc = auc_score(model, train,item_features=user_desc_rating_csr).mean()
test_auc = auc_score(model, test, train_interactions=train).mean()

#avaliar para observar a Recall
recall_train = recall_at_k(model, train , k=10, item_features=user_desc_rating_csr).mean()
recall_test = recall_at_k(model, test , k=10, item_features=user_desc_rating_csr, train_interactions=train,).mean()

#avaliar para observar a Mean Reciprocal rank
mrr_train = reciprocal_rank(model, train , item_features=user_desc_rating_csr).mean()
mrr_test = reciprocal_rank(model, test ,  item_features=user_desc_rating_csr, train_interactions=train,).mean()


print('Precision: train %.2f, test %.2f.' % (train_precision, test_precision))
print('AUC: train %.2f, test %.2f.' % (train_auc, test_auc))
print('Recall: train %.2f, test %.2f.' % (recall_train, recall_test))
print('MRR: train %.2f, test %.2f.' % (mrr_train, mrr_test))